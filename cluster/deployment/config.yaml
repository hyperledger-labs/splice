# Reference configuration options
cluster:
  nodePools:
    apps:
      minNodes: 0
      # A high max-nodes by default to support large deployments and hard migrations
      # Should be set to a lower number (currently 8) on CI clusters that do neither of those.
      maxNodes: 20
      nodeType: e2-standard-16
    infra:
      minNodes: 1
      maxNodes: 3
      nodeType: e2-standard-8
infra:
  prometheus:
    retentionDuration: "1y"
    retentionSize: "1500GB"
    storageSize: "2Ti"
  istio:
    enableIngressAccessLogging: true
# configs specific for the pulumi project
# will be applied to all the stacks in the project
pulumiProjectConfig:
  default:
    # skip helm installs and create only non helm resources.
    # this for example lets you create the cloud sql instances without having deployments using them, and restoring them from other sources
    installDataOnly: false
    isExternalCluster: false
    hasPublicInfo: false
    # For long running production clusters this flag can be set to false to remove pulumi dependencies between our apps.
    # This allows for much faster updates going all at once
    # We don't want this enabled for the initial deployments in ciclusters as the logs would become a lot noisier
    interAppsDependencies: true
    cloudSql:
      enabled: false
      protected: true
      # default tier is equivalent to "Standard" machine with 2 vCpus and 7.5GB RAM
      tier: 'db-custom-2-7680'
      # enable enterprise plus for better performance and faster maintenance
      enterprisePlus: false
    skipSynchronizerInitialization: true
  sv-runbook:
    cloudSql:
      enabled: false
pulumiStacks:
  default:
    parallelism: 64
  multi-validator:
    parallelism: 10
validator1:
  logging:
    level: DEBUG
# initial round used only on fresh initialization or resets
# first SV bootstraps the network with it, other SVs learn it via their sponsors
initialRound: 0
## in the form <pulumi-project>:
##                <settings>
## ex:
#  canton-network:
#    installDataOnly: true
monitoring:
  alerting:
    enableNoDataAlerts: false
    alerts:
      delegatelessContention:
        thresholdPerNamespace: 0.05
      trafficWaste:
        kilobytes: 1
        overMinutes: 5
      # confirmation requests correspond to new ledger submissions;
      # we alert if the rate is higher than expected to spot potential overload situations
      confirmationRequests:
        total:
          rate: 10
          overMinutes: 5
        perMember:
          rate: 2
          overMinutes: 5
      cloudSql:
        maintenance: false
      cometbft:
        expectedMaxBlocksPerSecond: 3.5
      loadTester:
        minRate: 0.95
      mediators:
        acknowledgementLagSeconds: 900
multiValidator:
  postgresPvcSize: '100Gi'
  resources:
    participant:
      requests:
        cpu: '1'
        memory: '8Gi'
      limits:
        cpu: '6'
        memory: '16Gi'
    validator:
      requests:
        cpu: '0.5'
        memory: '2400Mi'
      limits:
        cpu: '4'
        memory: '8Gi'
    postgres:
      requests:
        memory: '5Gi'
      limits:
        memory: '12Gi'
sv:
  participant:
    resources:
      requests:
        memory: '12Gi'
      limits:
        memory: '18Gi'
  scan:
    externalRateLimits:
      globalLimits:
        maxTokens: 2147483647
        tokensPerFill: 2147483647
        fillInterval: 60s
      rateLimits:
        - actions:
            - name: acs
              pathPrefix: /api/scan/v0/acs
            - name: client_ip
              clientIp: true
          limits:
            maxTokens: 10
            tokensPerFill: 5
            fillInterval: 60s
svs:
  default:
    logging:
      appsLogLevel: DEBUG
      cantonLogLevel: DEBUG
      cometbftLogLevel: debug
    participant:
      bftSequencerConnection: true
validators:
  validator-runbook:
    namespace: validator
    partyHint: 'digitalasset-testValidator-1'
    onboardingSecret: 'validatorsecret'
