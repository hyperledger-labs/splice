package org.lfdecentralizedtrust.splice.store

import com.daml.ledger.javaapi.data.{
  CreatedEvent,
  DamlRecord,
  ExercisedEvent,
  Int64,
  OffsetCheckpoint,
  Value,
}
import com.digitalasset.canton.concurrent.Threading
import com.digitalasset.canton.data.CantonTimestamp
import com.digitalasset.canton.util.MonadUtil
import com.digitalasset.daml.lf.data.Bytes
import com.google.rpc.status.Status
import com.google.rpc.status.Status.toJavaProto
import org.lfdecentralizedtrust.splice.codegen.java.splice.amulet.{Amulet, AppRewardCoupon}
import org.lfdecentralizedtrust.splice.environment.ledger.api.{
  ReassignmentUpdate,
  TransactionTreeUpdate,
  TreeUpdateOrOffsetCheckpoint,
}
import org.lfdecentralizedtrust.splice.util.DomainRecordTimeRange

import java.time.Instant
import java.util.Collections
import scala.concurrent.Future
import scala.jdk.CollectionConverters.*
import scala.jdk.OptionConverters.*
import UpdateHistory.UpdateHistoryResponse

class UpdateHistoryTest extends UpdateHistoryTestBase {

  import UpdateHistoryTestBase.*

  protected def updates(
      store: UpdateHistory,
      migrationId: Long = migration1,
  ): Future[Seq[UpdateHistoryResponse]] = {
    store
      .getAllUpdates(None, PageLimit.tryCreate(1000))
      .map(_.filter(_.migrationId == migrationId).map(_.update))
  }

  "UpdateHistory" should {

    "ingestion" should {

      "handle single create and query it by contract id" in {
        val store = mkStore()
        for {
          _ <- initStore(store)
          result <- store.lookupContractById(AppRewardCoupon.COMPANION)(
            new AppRewardCoupon.ContractId(cid1)
          )
          _ = result shouldBe None
          _ <- create(domain1, cid1, offset1, party1, store, time(1))
          result <- store.lookupContractById(AppRewardCoupon.COMPANION)(
            new AppRewardCoupon.ContractId(cid1)
          )
          _ = result shouldBe Some(
            appRewardCoupon(
              round = 0,
              provider = party1,
              contractId = cid1,
            )
          )
          updates <- updates(store)
        } yield checkUpdates(
          updates,
          Seq(
            ExpectedCreate(cid1, domain1)
          ),
        )
      }

      "handle transaction with non-standard data" in {
        val store = mkStore()
        for {
          _ <- initStore(store)
          expectedTree <- domain1.ingest(offset => {
            val party1 = mkPartyId("someParty").toProtoPrimitive
            val party2 = mkPartyId("someParty").toProtoPrimitive
            val contractId = nextCid()
            val id1 = Amulet.TEMPLATE_ID_WITH_PACKAGE_ID
            val someValue = new DamlRecord(
              new DamlRecord.Field("a", new Int64(42))
            )
            val effectiveAt = CantonTimestamp.Epoch.toInstant
            val recordTime = effectiveAt.plusSeconds(60)
            // Unlike transactions generated by various helper functions, this transaction has
            // a created and exercised event where every field is set to some (arbitrary) non-empty value,
            // including fields that are not normally not used in Splice, such as interfaces or contract keys.
            // This is to make sure serialization doesn't crash on such unexpected optional fields.
            mkTx(
              offset = offset,
              events = Seq(
                new CreatedEvent(
                  /*witnessParties*/ Seq(party1).asJava,
                  /*offset = */ 32,
                  /*nodeId = */ 53,
                  /*templateId*/ id1,
                  /*packageName*/ "somePackageName",
                  /*contractId*/ contractId,
                  /*arguments*/ someValue,
                  /*createdEventBlob*/ Bytes.assertFromString("00abcd").toByteString,
                  /*interfaceViews*/ new java.util.HashMap(Map(id1 -> someValue).asJava),
                  /*failedInterfaceViews*/ new java.util.HashMap(
                    Map(id1 -> toJavaProto(Status.of(1, "some message", Seq.empty))).asJava
                  ),
                  /*contractKey*/ Some[Value](someValue).toJava,
                  /*signatories*/ Seq(party1, party2).asJava,
                  /*observers*/ Seq(party1, party2).asJava,
                  /*createdAt*/ effectiveAt,
                  /*acsDelta*/ false,
                  /*representativePackageId*/ id1.getPackageId,
                ),
                new ExercisedEvent(
                  /*witnessParties*/ Seq(party1).asJava,
                  /*offset = */ 32,
                  /*nodeId = */ 52,
                  /*templateId*/ id1,
                  /*packageName*/ getPackageName(id1),
                  /*interfaceId*/ Some(id1).toJava,
                  /*contractId*/ contractId,
                  /*choice*/ "someChoice",
                  /*choiceArgument*/ someValue,
                  /*actingParties*/ List(party1).asJava,
                  /*consuming*/ false,
                  /*lastDescendedNodeId*/ Integer.valueOf(52),
                  /*exerciseResult*/ someValue,
                  /*implementedInterfaces*/ Seq.empty.asJava,
                  /*acsDelta*/ false,
                ),
              ),
              synchronizerId = domain1,
              effectiveAt = effectiveAt,
              recordTime = recordTime,
              workflowId = "SomeWorkflowId",
              commandId = "SomeCommandId",
            )
          })(store)
          updates <- updates(store)
        } yield {
          val expectedUpdates = Seq(
            UpdateHistoryResponse(
              TransactionTreeUpdate(expectedTree),
              domain1,
            )
          )
          val actualWithoutLostData = updates.map(withoutLostData(_, mode = LostInStoreIngestion))
          val expectedWithoutLostData =
            expectedUpdates.map(withoutLostData(_, mode = LostInStoreIngestion))
          actualWithoutLostData should contain theSameElementsInOrderAs expectedWithoutLostData
        }
      }

      "handle reassignments" in {
        implicit val store = mkStore()
        val c = appRewardCoupon(1, party1, contractId = cid1)
        for {
          _ <- initStore(store)
          _ <- domain1.create(
            c,
            txEffectiveAt = CantonTimestamp.Epoch.plusMillis(1).toInstant,
            recordTime = CantonTimestamp.Epoch.plusMillis(1).toInstant,
          )
          _ <- domain1.unassign(
            c -> domain2,
            reassignmentId1,
            1,
            CantonTimestamp.Epoch.plusMillis(2),
          )
          _ <- domain2.assign(c -> domain1, reassignmentId1, 1, CantonTimestamp.Epoch.plusMillis(3))
          _ <- domain2.exercise(
            c,
            None,
            "Archive",
            com.daml.ledger.javaapi.data.Unit.getInstance(),
            com.daml.ledger.javaapi.data.Unit.getInstance(),
            txEffectiveAt = CantonTimestamp.Epoch.plusMillis(4).toInstant,
            recordTime = CantonTimestamp.Epoch.plusMillis(4).toInstant,
          )
          updates <- updates(store)
        } yield checkUpdates(
          updates,
          Seq(
            ExpectedCreate(cid1, domain1),
            ExpectedUnassign(cid1, domain1, domain2),
            ExpectedAssign(cid1, domain1, domain2),
            ExpectedExercise(cid1, domain2, "Archive"),
          ),
        )
      }

      // Note: we do not really want to support multiple UpdateHistory instances ingesting
      // data for the same party from the same participant. We still want the UpdateHistory
      // to behave correctly if this happens by accident, however.
      "handle many stores concurrently ingesting the same stream" in {
        // 10 stores, all ingesting the same stream
        val stores = (1 to 10).toList.map(_ => mkStore())

        // One update stream with 10 updates
        val updateStreamElements = (1 to 10).toList.map(i =>
          i -> TransactionTreeUpdate(
            mkCreateTx(
              validOffset(i),
              Seq(appRewardCoupon(1, party1, contractId = validContractId(i))),
              defaultEffectiveAt.plusMillis(i.toLong),
              Seq(party1),
              domain1,
              "workflowId",
              recordTime = defaultEffectiveAt.plusMillis(i.toLong),
            )
          )
        )

        // Retry once on failure
        def retryOnceAfterAShortDelay[T](f: => Future[T]): Future[T] = {
          f.recoverWith { case _: Throwable =>
            Future(Threading.sleep(100)).flatMap(_ => f)
          }
        }

        for {
          // Initialize all stores in parallel
          _ <- Future.traverse(stores)(s => initStore(s))
          // Process one update at a time
          _ <- withoutRepeatedIngestionWarning(
            MonadUtil.sequentialTraverse(updateStreamElements) { case (i, update) =>
              logger.info(s"Processing update $i")
              // Ingest the same update on all stores in parallel
              Future.traverse(stores)(s =>
                // The first concurrent update is expected to fail on all but one store
                // with a uniqueness violation error (assuming the updates are really concurrent).
                // At the latest on the next retry, all stores should succeed ingesting the update
                // by figuring out that the given offset was already ingested.
                // In practice, the ingestion service would crash and restart after a "short delay".
                retryOnceAfterAShortDelay(
                  s.ingestionSink
                    .ingestUpdate(
                      domain1,
                      update,
                    )
                )
              )
            },
            maxCount = 199, // 10 stores x 10 updates x up to 2 retries per update
          )
          // Query all stores in parallel
          updatesList <- Future.traverse(stores)(s => updates(s))
        } yield {
          // All stores should return all 10 updates
          updatesList.foreach(updates =>
            checkUpdates(
              updates,
              updateStreamElements.map(u => ExpectedCreate(validContractId(u._1), domain1)),
            )
          )

          succeed
        }
      }

      "two stores: different parties" in {
        val store1 = mkStore(party1, migration1, participant1)
        val store2 = mkStore(party2, migration1, participant1)

        for {
          _ <- initStore(store1)
          _ <- initStore(store2)
          _ <- create(domain1, cid1, offset1, party1, store1, time(1))
          _ <- create(domain1, cid2, offset2, party2, store2, time(2))
          updates1 <- updates(store1)
          updates2 <- updates(store2)
        } yield {
          checkUpdates(
            updates1,
            Seq(
              ExpectedCreate(cid1, domain1)
            ),
          )
          checkUpdates(
            updates2,
            Seq(
              ExpectedCreate(cid2, domain1)
            ),
          )
        }
      }

      "two stores: different participant" in {
        val store1 = mkStore(party1, migration1, participant1)
        val store2 = mkStore(party1, migration1, participant2)

        for {
          _ <- initStore(store1)
          _ <- initStore(store2)
          // Note: same offset (offsets are participant-specific)
          _ <- create(domain1, cid1, offset1, party1, store1, time(1))
          _ <- create(domain1, cid2, offset1, party1, store2, time(2))
          updates1 <- updates(store1)
          updates2 <- updates(store2)
        } yield {
          checkUpdates(
            updates1,
            Seq(
              ExpectedCreate(cid1, domain1)
            ),
          )
          checkUpdates(
            updates2,
            Seq(
              ExpectedCreate(cid2, domain1)
            ),
          )
        }
      }

      "two stores: different store_name, same participant" in {
        val store1 = mkStore(party1, migration1, participant1, "store2")
        val store2 = mkStore(party1, migration1, participant1, "store3")

        for {
          _ <- initStore(store1)
          _ <- initStore(store2)
          // Note: same offset (offsets are participant-specific)
          _ <- create(domain1, cid1, offset1, party1, store1, time(1))
          _ <- create(domain1, cid2, offset1, party1, store2, time(2))
          updates1 <- updates(store1)
          updates2 <- updates(store2)
        } yield {
          checkUpdates(
            updates1,
            Seq(
              ExpectedCreate(cid1, domain1)
            ),
          )
          checkUpdates(
            updates2,
            Seq(
              ExpectedCreate(cid2, domain1)
            ),
          )
        }
      }

      "two stores: different migration indices" in {
        val store1 = mkStore(party1, migration1, participant1)
        val store2 = mkStore(party1, migration2, participant1)

        for {
          _ <- initStore(store1)
          _ <- initStore(store2)
          // Note: same offset (offsets are not preserved across hard domain migrations)
          _ <- create(domain1, cid1, offset1, party1, store1, time(1))
          _ <- create(domain1, cid2, offset1, party1, store2, time(2))
          updates1 <- updates(store1, migration1)
          updates2 <- updates(store2, migration2)
        } yield {
          checkUpdates(
            updates1,
            Seq(
              ExpectedCreate(cid1, domain1)
            ),
          )
          checkUpdates(
            updates2,
            Seq(
              ExpectedCreate(cid2, domain1)
            ),
          )
        }
      }

      "one store: different domains" in {
        val store1 = mkStore(party1, migration1, participant1)

        for {
          _ <- initStore(store1)
          // Note: the two contracts can share a record time (record times are not unique across domains)
          _ <- create(domain1, cid1, offset1, party1, store1, time(1))
          _ <- create(domain2, cid2, offset2, party1, store1, time(2))
          updates1 <- updates(store1)
        } yield {
          checkUpdates(
            updates1,
            Seq(
              ExpectedCreate(cid1, domain1),
              ExpectedCreate(cid2, domain2),
            ),
          )
        }
      }

      "pagination works" in {
        val storeMigrationId1 = mkStore(party1, migration1, participant1)
        val storeMigrationId2 = mkStore(party1, migration2, participant1)

        val updates = (1 to 10).toList.map(i =>
          TransactionTreeUpdate(
            mkCreateTx(
              validOffset(i),
              Seq(appRewardCoupon(1, party1, contractId = validContractId(i))),
              defaultEffectiveAt.plusMillis(i.toLong),
              Seq(party1),
              domain1,
              s"workflowId#$i",
              recordTime = defaultEffectiveAt.plusMillis(i.toLong),
            )
          )
        )

        def allHistoryPaginated(
            store: UpdateHistory,
            after: Option[(Long, Instant)],
            acc: Seq[TransactionTreeUpdate],
        ): Seq[TransactionTreeUpdate] = {
          val result =
            store
              .getAllUpdates(
                after.map { case (migrationId, recordTime) =>
                  (migrationId, CantonTimestamp.assertFromInstant(recordTime))
                },
                PageLimit.tryCreate(1),
              )
              .futureValue
          result.lastOption match {
            case None => acc // done
            case Some(TreeUpdateWithMigrationId(last, migrationId)) =>
              last.update match {
                case tree: TransactionTreeUpdate =>
                  allHistoryPaginated(
                    store,
                    Some((migrationId, tree.tree.getRecordTime)),
                    acc :+ tree,
                  )
                case ReassignmentUpdate(transfer) =>
                  allHistoryPaginated(
                    store,
                    Some((migrationId, transfer.recordTime.toInstant)),
                    acc,
                  )
              }
          }
        }

        for {
          _ <- initStore(storeMigrationId1)
          _ <- withoutRepeatedIngestionWarning(
            MonadUtil.sequentialTraverse(updates) { update =>
              storeMigrationId1.ingestionSink
                .ingestUpdate(
                  domain1,
                  update,
                )
            },
            maxCount = updates.size,
          )
          _ <- initStore(storeMigrationId2)
          // insert the same transactions but in migration id 2,
          _ <- withoutRepeatedIngestionWarning(
            MonadUtil.sequentialTraverse(updates) { update =>
              storeMigrationId2.ingestionSink
                .ingestUpdate(
                  domain1,
                  update,
                )
            },
            maxCount = updates.size,
          )
          all <- storeMigrationId1.getAllUpdates(
            None,
            PageLimit.tryCreate(1000),
          )
          all2 <- storeMigrationId2.getAllUpdates(
            None,
            PageLimit.tryCreate(1000),
          )
        } yield {
          // It doesn't matter through which store we query since the migration id only matters for ingestion
          all shouldBe all2
          allHistoryPaginated(storeMigrationId1, None, Seq.empty) should be(
            all.map(_.update.update)
          )
          val expected = ((1 to 10).map(i =>
            (1L, CantonTimestamp.assertFromInstant(defaultEffectiveAt.plusMillis(i.toLong)))
          ) ++
            (1 to 10).map(i =>
              (2L, CantonTimestamp.assertFromInstant(defaultEffectiveAt.plusMillis(i.toLong)))
            ))
          all.map { case TreeUpdateWithMigrationId(u, migrationId) =>
            (migrationId, u.update.recordTime)
          } shouldBe expected
        }
      }

      "getRecordTimeRange works" in {
        val storeMigrationId1 = mkStore(party1, migration1, participant1)
        val storeMigrationId2 = mkStore(party1, migration2, participant1)

        for {
          _ <- initStore(storeMigrationId1)
          _ <- storeMigrationId1
            .getRecordTimeRange(1)
            .map(_ shouldBe Map.empty)
          _ <-
            MonadUtil.sequentialTraverse(1 to 10)(i =>
              create(
                domain1,
                validContractId(i),
                validOffset(i),
                party1,
                storeMigrationId1,
                time(i),
              )
            )
          _ <- initStore(storeMigrationId2)
          _ <- storeMigrationId1
            .getRecordTimeRange(1)
            .map(
              _ shouldBe Map(
                domain1 -> DomainRecordTimeRange(
                  CantonTimestamp.assertFromInstant(defaultEffectiveAt.plusMillis(1)),
                  CantonTimestamp.assertFromInstant(defaultEffectiveAt.plusMillis(10)),
                )
              )
            )
          _ <- storeMigrationId2
            .getRecordTimeRange(1)
            .map(
              _ shouldBe Map(
                domain1 -> DomainRecordTimeRange(
                  CantonTimestamp.assertFromInstant(defaultEffectiveAt.plusMillis(1)),
                  CantonTimestamp.assertFromInstant(defaultEffectiveAt.plusMillis(10)),
                )
              )
            )
          _ <- storeMigrationId2.getRecordTimeRange(2).map(_ shouldBe Map())

          // We exclude CantonTimestamp.MinValue which are the transactions imported as part of the HDM as opposed to transactions actually sequenced.
          _ <- create(
            domain1,
            validContractId(11),
            validOffset(11),
            party1,
            storeMigrationId2,
            CantonTimestamp.MinValue,
          )

          _ <- storeMigrationId2.getRecordTimeRange(2).map(_ shouldBe Map())

          // insert a transaction after CantonTimestamp.MinValue which now advances the record timestamp boundaries.
          _ <- create(
            domain1,
            validContractId(12),
            validOffset(12),
            party1,
            storeMigrationId2,
            time(12),
          )

          _ <- storeMigrationId2
            .getRecordTimeRange(2)
            .map(
              _ shouldBe Map(
                domain1 -> DomainRecordTimeRange(
                  CantonTimestamp.assertFromInstant(defaultEffectiveAt.plusMillis(12)),
                  CantonTimestamp.assertFromInstant(defaultEffectiveAt.plusMillis(12)),
                )
              )
            )

          // ingest on different domain
          _ <- create(
            domain2,
            validContractId(13),
            validOffset(13),
            party1,
            storeMigrationId2,
            time(0),
          )
          _ <- storeMigrationId2
            .getRecordTimeRange(2)
            .map(
              _ shouldBe Map(
                domain1 -> DomainRecordTimeRange(
                  CantonTimestamp.assertFromInstant(defaultEffectiveAt.plusMillis(12)),
                  CantonTimestamp.assertFromInstant(defaultEffectiveAt.plusMillis(12)),
                ),
                domain2 -> DomainRecordTimeRange(
                  CantonTimestamp.assertFromInstant(defaultEffectiveAt),
                  CantonTimestamp.assertFromInstant(defaultEffectiveAt),
                ),
              )
            )

        } yield succeed
      }

      "offset checkpoints can be ingested" in {
        val store = mkStore()
        for {
          _ <- initStore(store)
          o1 <- store.lookupLastIngestedOffset()
          _ = o1 shouldBe None
          _ <- store.testIngestionSink.ingestUpdate(
            TreeUpdateOrOffsetCheckpoint.Checkpoint(
              new OffsetCheckpoint(
                5,
                Collections.emptyList(),
              )
            )
          )
          o2 <- store.lookupLastIngestedOffset()
          _ = o2 shouldBe Some(5)
        } yield succeed
      }

    }

    "getImportUpdates" should {

      "return consistent ids" in {
        val storeA1 = mkStore(party1, migration1, participant1)
        val storeA2 = mkStore(party1, migration2, participant1)
        val storeB1 = mkStore(party2, migration1, participant1)
        val storeB2 = mkStore(party2, migration2, participant1)
        for {
          // Ingest the same update into two stores.
          _ <- initStore(storeA1)
          _ <- initStore(storeB1)
          txA <- create(domain1, cid1, offset1, party1, storeA1, time(1), Seq(party2))
          txB <- create(domain1, cid1, offset1, party1, storeB1, time(1), Seq(party2))

          // Ingest the import update corresponding to the above in the next migration.
          _ <- initStore(storeA2)
          _ <- initStore(storeB2)
          itA <- importUpdate(txA, offset1, storeA2)
          itB <- importUpdate(txB, offset1, storeB2)

          resultA <- storeA2.getImportUpdates(migration2, "", PageLimit.tryCreate(1000))
          resultB <- storeB2.getImportUpdates(migration2, "", PageLimit.tryCreate(1000))
        } yield {
          // Updates generated by test code should have random update ids (otherwise this test is not doing anything)
          txA.getUpdateId should not be txB.getUpdateId
          itA.getUpdateId should not be itB.getUpdateId

          // Updates queried through the `getImportUpdates` method should have consistent update ids
          val updateA = resultA.loneElement.update.update.asInstanceOf[TransactionTreeUpdate]
          val updateB = resultB.loneElement.update.update.asInstanceOf[TransactionTreeUpdate]
          updateA.updateId shouldBe updateB.updateId

          // ... and consistent event ids
          val eventIdsA = updateA.tree.getRootNodeIds.asScala
          val eventIdsB = updateB.tree.getRootNodeIds.asScala
          eventIdsA shouldBe eventIdsB
        }
      }

      "sort and limit correctly" in {
        val store1 = mkStore(party1, migration1, participant1)
        val store2 = mkStore(party1, migration2, participant1)
        for {
          // Ingest some updates with random contract ids
          _ <- initStore(store1)
          tx3 <- create(domain1, "c3", validOffset(1), party1, store1, time(1))
          tx1 <- create(domain1, "c1", validOffset(2), party1, store1, time(2))
          tx4 <- create(domain1, "c4", validOffset(3), party1, store1, time(3))
          tx2 <- create(domain1, "c2", validOffset(4), party1, store1, time(4))

          // Ingest the import update corresponding to the above in the next migration.
          _ <- initStore(store2)
          _ <- importUpdate(tx4, validOffset(1), store2)
          _ <- importUpdate(tx1, validOffset(2), store2)
          _ <- importUpdate(tx3, validOffset(3), store2)
          _ <- importUpdate(tx2, validOffset(4), store2)

          result0 <- store2.getImportUpdates(migration2, "", PageLimit.tryCreate(1000))
          result2 <- store2.getImportUpdates(migration2, "c2", PageLimit.tryCreate(1000))
          result4 <- store2.getImportUpdates(migration2, "c4", PageLimit.tryCreate(1000))
          result5 <- store2.getImportUpdates(migration2, "c1", PageLimit.tryCreate(2))
        } yield {
          result0.map(_.update.update.updateId) shouldBe Seq("c1", "c2", "c3", "c4")
          result2.map(_.update.update.updateId) shouldBe Seq("c3", "c4")
          result4 shouldBe empty
          result5.map(_.update.update.updateId) shouldBe Seq("c2", "c3")
        }
      }

      "return consistent data" in {
        val store1 = mkStore(storeName = "store1")
        val store2 = mkStore(storeName = "store2")
        val party1 = mkPartyId("someParty1").toProtoPrimitive
        val party2 = mkPartyId("someParty2").toProtoPrimitive
        val contractId1 = nextCid()
        val contractId2 = nextCid()
        val id1 = new com.daml.ledger.javaapi.data.Identifier(
          "somePackageId1",
          "someModuleName1",
          "someEntityName1",
        )
        val someValue = new DamlRecord(
          new DamlRecord.Field("a", new Int64(42))
        )
        val effectiveAt = CantonTimestamp.MinValue.toInstant
        val recordTime = CantonTimestamp.MinValue.toInstant
        for {
          _ <- initStore(store1)
          _ <- initStore(store2)
          // Store1 ingests all import updates in a single transaction
          _ <- domain1.ingest(offset => {
            mkTx(
              offset = offset,
              events = Seq(
                new CreatedEvent(
                  /*witnessParties*/ Seq(party1).asJava,
                  /*offset = */ 32,
                  /*nodeId = */ 53,
                  /*templateId*/ id1,
                  /*packageName*/ "somePackageName",
                  /*contractId*/ contractId1,
                  /*arguments*/ someValue,
                  /*createdEventBlob*/ Bytes.assertFromString("00abcd").toByteString,
                  /*interfaceViews*/ new java.util.HashMap(Map(id1 -> someValue).asJava),
                  /*failedInterfaceViews*/ new java.util.HashMap(
                    Map(id1 -> toJavaProto(Status.of(1, "some message", Seq.empty))).asJava
                  ),
                  /*contractKey*/ Some[Value](someValue).toJava,
                  /*signatories*/ Seq(party1).asJava,
                  /*observers*/ Seq(party1, party2).asJava,
                  /*createdAt*/ effectiveAt,
                  /*acsDelta*/ false,
                  /*representativePackageId*/ id1.getPackageId,
                ),
                new CreatedEvent(
                  /*witnessParties*/ Seq(party1).asJava,
                  /*offset = */ 33,
                  /*nodeId = */ 54,
                  /*templateId*/ id1,
                  /*packageName*/ "somePackageName",
                  /*contractId*/ contractId2,
                  /*arguments*/ someValue,
                  /*createdEventBlob*/ Bytes.assertFromString("00abcd").toByteString,
                  /*interfaceViews*/ new java.util.HashMap(Map(id1 -> someValue).asJava),
                  /*failedInterfaceViews*/ new java.util.HashMap(
                    Map(id1 -> toJavaProto(Status.of(1, "some message", Seq.empty))).asJava
                  ),
                  /*contractKey*/ Some[Value](someValue).toJava,
                  /*signatories*/ Seq(party2).asJava,
                  /*observers*/ Seq(party1, party2).asJava,
                  /*createdAt*/ effectiveAt,
                  /*acsDelta*/ false,
                  /*representativePackageId*/ id1.getPackageId,
                ),
              ),
              synchronizerId = domain1,
              effectiveAt = effectiveAt,
              recordTime = recordTime,
              workflowId = "WorkflowId1",
              commandId = "CommandId1",
            )
          })(store1)
          // Store2 ingests import updates in individual transactions, and uses different
          // values for offsets, node IDs, and workflow IDs.
          _ <- domain1.ingest(offset => {
            mkTx(
              offset = offset,
              events = Seq(
                new CreatedEvent(
                  /*witnessParties*/ Seq(party1).asJava,
                  /*offset = */ 132,
                  /*nodeId = */ 153,
                  /*templateId*/ id1,
                  /*packageName*/ "somePackageName",
                  /*contractId*/ contractId1,
                  /*arguments*/ someValue,
                  /*createdEventBlob*/ Bytes.assertFromString("00abcd").toByteString,
                  /*interfaceViews*/ new java.util.HashMap(Map(id1 -> someValue).asJava),
                  /*failedInterfaceViews*/ new java.util.HashMap(
                    Map(id1 -> toJavaProto(Status.of(1, "some message", Seq.empty))).asJava
                  ),
                  /*contractKey*/ Some[Value](someValue).toJava,
                  /*signatories*/ Seq(party1).asJava,
                  /*observers*/ Seq(party1, party2).asJava,
                  /*createdAt*/ effectiveAt,
                  /*acsDelta*/ false,
                  /*representativePackageId*/ id1.getPackageId,
                )
              ),
              synchronizerId = domain1,
              effectiveAt = effectiveAt,
              recordTime = recordTime,
              workflowId = "WorkflowId2",
              commandId = "CommandId2",
            )
          })(store2)
          _ <- domain1.ingest(offset => {
            mkTx(
              offset = offset,
              events = Seq(
                new CreatedEvent(
                  /*witnessParties*/ Seq(party1).asJava,
                  /*offset = */ 133,
                  /*nodeId = */ 154,
                  /*templateId*/ id1,
                  /*packageName*/ "somePackageName",
                  /*contractId*/ contractId2,
                  /*arguments*/ someValue,
                  /*createdEventBlob*/ Bytes.assertFromString("00abcd").toByteString,
                  /*interfaceViews*/ new java.util.HashMap(Map(id1 -> someValue).asJava),
                  /*failedInterfaceViews*/ new java.util.HashMap(
                    Map(id1 -> toJavaProto(Status.of(1, "some message", Seq.empty))).asJava
                  ),
                  /*contractKey*/ Some[Value](someValue).toJava,
                  /*signatories*/ Seq(party2).asJava,
                  /*observers*/ Seq(party1, party2).asJava,
                  /*createdAt*/ effectiveAt,
                  /*acsDelta*/ false,
                  /*representativePackageId*/ id1.getPackageId,
                )
              ),
              synchronizerId = domain1,
              effectiveAt = effectiveAt,
              recordTime = recordTime,
              workflowId = "WorkflowId3",
              commandId = "CommandId3",
            )
          })(store2)
          updates1 <- store1.getImportUpdates(migration1, "", PageLimit.Max)
          updates2 <- store2.getImportUpdates(migration1, "", PageLimit.Max)
        } yield {
          updates1 should have size 2
          updates2 should have size 2
          // getImportUpdates already normalizes data, no need to call `withoutLostData`
          updates1 should contain theSameElementsInOrderAs updates2
        }
      }
    }

    "getHighestKnownMigrationId" should {

      "return None if there is no store" in {
        UpdateHistory.getHighestKnownMigrationId(storage).futureValue shouldBe None
      }

      "return None if the store is not initialized" in {
        mkStore()
        for {
          migrationId <- UpdateHistory.getHighestKnownMigrationId(storage)
        } yield {
          migrationId shouldBe None
        }
      }

      "return a migration id if there is an initialized store" in {
        val store = mkStore()
        for {
          _ <- initStore(store)
          migrationId <- UpdateHistory.getHighestKnownMigrationId(storage)
        } yield {
          migrationId shouldBe Some(migration1)
        }
      }

      "return the highest migration id if there are multiple initialized stores" in {
        val store1 = mkStore(party1, migration1, participant1)
        val store2 = mkStore(party1, migration2, participant1)
        for {
          _ <- initStore(store1)
          _ <- initStore(store2)
          migrationId <- UpdateHistory.getHighestKnownMigrationId(storage)
        } yield {
          migrationId shouldBe Some(migration2)
        }
      }
    }
  }
}
